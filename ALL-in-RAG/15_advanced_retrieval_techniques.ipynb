{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86f8538c",
   "metadata": {},
   "source": [
    "# 检索进阶\n",
    "\n",
    "在基础的 RAG 流程中，依赖向量相似度从知识库中检索信息。\n",
    "\n",
    "这种方法存在一些固有的局限性，例如最相关的文档不总是在检索结果的顶端，以及语义理解的偏差等。\n",
    "\n",
    "为了构建更强大、更精准的生产级 RAG 应用，需要引入更高级的检索技术。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa6f5af",
   "metadata": {},
   "source": [
    "## 重排序 (Re-ranking)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d94767a",
   "metadata": {},
   "source": [
    "### RRF (Reciprocal Rank Fusion)\n",
    "\n",
    "RRF 是一种简单而有效的零样本重排方法，不依赖于任何模型训练，\n",
    "\n",
    "纯粹基于文档在多个不同检索器结果列表中的排名来计算最终分数。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda823bc",
   "metadata": {},
   "source": [
    "### RankLLM / LLM-based Reranker\n",
    "\n",
    "![](images/4_5_2.webp)\n",
    "\n",
    "RankLLM **代表了一类**直接利用大型语言模型本身来进行重排的方法，这种方法通过一个精心设计的提示词来实现。\n",
    "\n",
    "该提示词会包含用户的查询和一系列候选文档（通常是文档的摘要或关键部分），\n",
    "\n",
    "然后要求 LLM 以特定格式（如 JSON）输出一个排序后的文档列表，并给出每个文档的相关性分数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc951d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "\n",
    "以下是一个文档列表，每个文档都有一个编号和摘要。同时提供一个问题。请根据问题，按相关性顺序列出您认为需要查阅的文档编号，并给出相关性分数（1-10分）。请不要包含与问题无关的文档。\n",
    "\n",
    "示例格式:\n",
    "文档 1: <文档1的摘要>\n",
    "文档 2: <文档2的摘要>\n",
    "...\n",
    "文档 10: <文档10的摘要>\n",
    "\n",
    "问题: <用户的问题>\n",
    "\n",
    "回答:\n",
    "Doc: 9, Relevance: 7\n",
    "Doc: 3, Relevance: 4\n",
    "Doc: 7, Relevance: 3\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cd12a7",
   "metadata": {},
   "source": [
    "### Cross-Encoder 重排\n",
    "\n",
    "Cross-Encoder（交叉编码器）是将查询（Query）和每个候选文档（Document）拼接成一个单一的输入。\n",
    "\n",
    "**[CLS] query [SEP] document [SEP]**\n",
    "\n",
    "将其输入到一个预训练的 Transformer 模型（如 BERT）中，\n",
    "\n",
    "最终输出一个单一的分数（通常在 0 到 1 之间），这个分数直接代表了文档与查询的相关性。\n",
    "\n",
    "![](images/4_5_3.svg)\n",
    "\n",
    "1. 初步检索：搜索引擎首先从知识库中召回一个初始的文档列表（例如，前 50 篇）。\n",
    "\n",
    "2. 逐一评分：对于列表中的每一篇文档，系统都将其与原始查询配对，然后发送给 Cross-Encoder 模型。\n",
    "\n",
    "3. 独立推理：模型对每个“查询-文档”对进行一次完整的、独立的推理计算，得出一个精确的相关性分数。\n",
    "\n",
    "4. 返回重排结果：系统根据这些新的分数对文档列表进行重新排序，并将最终结果返回给用户。\n",
    "\n",
    "常见的 Cross-Encoder 模型包括 ms-marco-MiniLM-L-12-v2、ms-marco-TinyBERT-L-2-v2 等。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39091245",
   "metadata": {},
   "source": [
    "### ColBERT 重排\n",
    "\n",
    "一种创新的重排模型，采用了一种“后期交互”机制，在 Cross-Encoder 的高精度和双编码器（Bi-Encoder）的高效率之间取得了平衡。\n",
    "\n",
    "1. 独立编码：ColBERT 分别为查询（Query）和文档（Document）中的每个 Token 生成上下文相关的嵌入向量。\n",
    "    \n",
    "    - 这一步是独立完成的，可以预先计算并存储文档的向量，从而加快查询速度。\n",
    "\n",
    "2. 后期交互：在查询时，模型会计算查询中每个 Token 的向量与文档中每个 Token 向量之间的最大相似度（MaxSim）。\n",
    "\n",
    "3. 分数聚合：最后，将查询中所有 Token 得到的最大相似度分数相加，得到最终的相关性总分。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc894e8",
   "metadata": {},
   "source": [
    "### 对比\n",
    "\n",
    "| 特性 | RRF | RankLLM | Cross-Encoder | ColBERT |\n",
    "| :--- | :--- | :--- | :--- | :--- |\n",
    "| **核心机制** | 融合多个排名 | LLM 推理，生成排序列表 | 联合编码查询与文档，计算单一相关分 | 独立编码，后期交互 |\n",
    "| **计算成本** | 低（简单数学计算） | 中 (API 费用与延迟) | 高（N次模型推理） | 中（向量点积计算） |\n",
    "| **交互粒度** | 无（仅排名） | 概念/语义级 | 句子级（Query-Doc Pair） | Token 级 |\n",
    "| **适用场景** | 多路召回结果融合 | 高价值语义理解场景 | Top-K 精排 | Top-K 重排 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dea2c94",
   "metadata": {},
   "source": [
    "## 压缩 (Compression)\n",
    "\n",
    "初步检索到的文档块（Chunks）虽然整体上与查询相关，但可能包含大量无关的“噪音”文本。\n",
    "\n",
    "这些未经处理的、冗长的上下文直接提供给 LLM，不仅会增加 API 调用的成本和延迟，还可能因为信息过载而降低最终生成答案的质量。\n",
    "\n",
    "压缩的目标就是对检索到的内容进行“压缩”和“提炼”，只保留与用户查询最直接相关的信息。\n",
    "\n",
    "**实现方式：**\n",
    "\n",
    "- 内容提取：从文档中只抽出与查询相关的句子或段落。\n",
    "\n",
    "- 文档过滤：完全丢弃那些虽然被初步召回，但经过更精细判断后认为不相关的整个文档。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697ec20a",
   "metadata": {},
   "source": [
    "### LangChain 的 DocumentCompressor\n",
    "\n",
    "- LLMChainExtractor: 这是最直接的压缩方式。遍历每个文档，并利用 LLM 来判断并提取出其中与查询相关的部分。\n",
    "\n",
    "- LLMChainFilter: 同样使用 LLM，判断整个文档是否与查询相关，如果相关，则保留整个文档；如果不相关，则直接丢弃。\n",
    "\n",
    "- EmbeddingsFilter: 计算查询和每个文档的嵌入向量之间的相似度，只保留那些相似度超过预设阈值的文档。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f0dc76",
   "metadata": {},
   "source": [
    "### 自定义重排器与压缩管道\n",
    "\n",
    " LangChain + ColBERT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54809138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ColBERT模型加载完成\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents import BaseDocumentCompressor\n",
    "from typing import Sequence\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "class ColBERTReranker(BaseDocumentCompressor):\n",
    "    \"\"\"ColBERT重排器\"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        model_name = \"models/bert/bert-base-uncased\"\n",
    "\n",
    "        # 加载模型和分词器\n",
    "        object.__setattr__(self, 'tokenizer', AutoTokenizer.from_pretrained(model_name))\n",
    "        object.__setattr__(self, 'model', AutoModel.from_pretrained(model_name))\n",
    "        self.model.eval()\n",
    "        print(f\"ColBERT模型加载完成\")\n",
    "\n",
    "    def encode_text(self, texts):\n",
    "        \"\"\"ColBERT文本编码\"\"\"\n",
    "        inputs = self.tokenizer(\n",
    "            texts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=128\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "\n",
    "        embeddings = outputs.last_hidden_state\n",
    "        embeddings = F.normalize(embeddings, p=2, dim=-1)\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "    def calculate_colbert_similarity(self, query_emb, doc_embs, query_mask, doc_masks):\n",
    "        \"\"\"ColBERT相似度计算（MaxSim操作）\"\"\"\n",
    "        scores = []\n",
    "\n",
    "        for i, doc_emb in enumerate(doc_embs):\n",
    "            doc_mask = doc_masks[i:i+1]\n",
    "\n",
    "            # 计算相似度矩阵\n",
    "            similarity_matrix = torch.matmul(query_emb, doc_emb.unsqueeze(0).transpose(-2, -1))\n",
    "\n",
    "            # 应用文档mask\n",
    "            doc_mask_expanded = doc_mask.unsqueeze(1)\n",
    "            similarity_matrix = similarity_matrix.masked_fill(~doc_mask_expanded.bool(), -1e9)\n",
    "\n",
    "            # MaxSim操作\n",
    "            max_sim_per_query_token = similarity_matrix.max(dim=-1)[0]\n",
    "\n",
    "            # 应用查询mask\n",
    "            query_mask_expanded = query_mask.unsqueeze(0)\n",
    "            max_sim_per_query_token = max_sim_per_query_token.masked_fill(~query_mask_expanded.bool(), 0)\n",
    "\n",
    "            # 求和得到最终分数\n",
    "            colbert_score = max_sim_per_query_token.sum(dim=-1).item()\n",
    "            scores.append(colbert_score)\n",
    "\n",
    "        return scores\n",
    "\n",
    "    def compress_documents(\n",
    "        self,\n",
    "        documents: Sequence[Document],\n",
    "        query: str,\n",
    "        callbacks=None,\n",
    "    ) -> Sequence[Document]:\n",
    "        \"\"\"对文档进行ColBERT重排序\"\"\"\n",
    "        if len(documents) == 0:\n",
    "            return documents\n",
    "\n",
    "        # 编码查询\n",
    "        query_inputs = self.tokenizer(\n",
    "            [query],\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=128\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            query_outputs = self.model(**query_inputs)\n",
    "            query_embeddings = F.normalize(query_outputs.last_hidden_state, p=2, dim=-1)\n",
    "\n",
    "        # 编码文档\n",
    "        doc_texts = [doc.page_content for doc in documents]\n",
    "        doc_inputs = self.tokenizer(\n",
    "            doc_texts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=128\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            doc_outputs = self.model(**doc_inputs)\n",
    "            doc_embeddings = F.normalize(doc_outputs.last_hidden_state, p=2, dim=-1)\n",
    "\n",
    "        # 计算ColBERT相似度\n",
    "        scores = self.calculate_colbert_similarity(\n",
    "            query_embeddings,\n",
    "            doc_embeddings,\n",
    "            query_inputs['attention_mask'],\n",
    "            doc_inputs['attention_mask']\n",
    "        )\n",
    "\n",
    "        # 排序并返回前5个\n",
    "        scored_docs = list(zip(documents, scores))\n",
    "        scored_docs.sort(key=lambda x: x[1], reverse=True)\n",
    "        reranked_docs = [doc for doc, _ in scored_docs[:5]]\n",
    "\n",
    "        return reranked_docs\n",
    "    \n",
    "reranker = ColBERTReranker()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757f4883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化大模型\n",
    "from langchain_deepseek import ChatDeepSeek\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "hf_bge_embeddings = HuggingFaceEmbeddings(model_name=\"models/bge/bge-small-zh-v1.5\")\n",
    "\n",
    "llm = ChatDeepSeek(\n",
    "    model=\"deepseek-chat\", \n",
    "    temperature=0.1, \n",
    "    api_key=os.getenv(\"DEEPSEEK_API_KEY\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1fc702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载和处理文档\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = TextLoader(\"data/ai.txt\", encoding=\"utf-8\")\n",
    "documents = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "docs = text_splitter.split_documents(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927e2c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建向量存储和基础检索器\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "vectorstore = FAISS.from_documents(docs, hf_bge_embeddings)\n",
    "base_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 20})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c75bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM 压缩器\n",
    "from langchain_classic.retrievers.document_compressors import LLMChainExtractor\n",
    "\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6174f692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用DocumentCompressorPipeline组装压缩管道\n",
    "# 流程: ColBERT重排 -> LLM压缩\n",
    "from langchain_classic.retrievers.document_compressors import DocumentCompressorPipeline\n",
    "\n",
    "pipeline_compressor = DocumentCompressorPipeline(transformers=[reranker, compressor])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589c960a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建最终的压缩检索器\n",
    "from langchain_classic.retrievers.contextual_compression import ContextualCompressionRetriever\n",
    "\n",
    "final_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=pipeline_compressor,\n",
    "    base_retriever=base_retriever\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdbfd10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== 开始执行查询 ====================\n",
      "查询: AI还有哪些缺陷需要克服？\n",
      "\n",
      "--- (1) 基础检索结果 (Top 20) ---\n",
      "  [1] 一个比较明显的问题是，AI生成内容虽然已非常流畅，但提供的信息很多时候还是不准确。5月，日本研究人员在德国《先进科学》杂志发表的一项研究成果中指出，这一问题与人类的语言障碍——失语症类似。\n",
      "\n",
      "    ...\n",
      "\n",
      "  [2] 行业巨头谷歌公司也没闲着。该公司在5月推出整体性能和智能推理能力均较以往版本大幅提升的多个“双子座2.5”系列模型，并发布了多个多模态模型，如图像生成模型Imagen 4和视频生成模型Veo 3，具备...\n",
      "\n",
      "  [3] 业界也确实在努力从不同角度去寻求优化大模型的解决方案。中国科学院自动化研究所联合鹏城实验室提出了一种高效推理策略AutoThink，可让大模型实现自主切换思考模式，避免“过度思考”。\n",
      "\n",
      "    据研究...\n",
      "\n",
      "  [4] 一些国家已在积极尝试通过优化政策、法规来营造更好的AI创新环境。日本参议院全体会议5月28日以多数赞成票通过该国首部专门针对AI的法律，旨在促进AI相关技术研发和应用并防止其滥用。依据这部《人工智能相...\n",
      "\n",
      "  [5] 5月，全球多家科技公司发布新的大模型，它们在语义理解、多模态等方面进一步提升，人工智能（AI）的能力边界在不断扩大。随着无人驾驶、机器人等技术借助AI快速进化并逐步投入市场，不少国家通过推进法规建设、...\n",
      "\n",
      "\n",
      "--- (2) 管道压缩后结果 (ColBERT重排 + LLM压缩) ---\n",
      "  [1] 一个比较明显的问题是，AI生成内容虽然已非常流畅，但提供的信息很多时候还是不准确。\n",
      "大模型在出现严重错误时仍表达流畅，这与感觉性失语症的症状有相似之处，即说话流利却总说不出什么意思。\n",
      "它们可能被锁定在一种僵化的内部模式中，限制其灵活运用所储存知识。\n",
      "\n",
      "  [2] 中国科学院自动化研究所联合鹏城实验室提出了一种高效推理策略AutoThink，可让大模型实现自主切换思考模式，避免“过度思考”。\n",
      "\n",
      "    据研究人员介绍，AutoThink提供了一种简单而有效的推理新范式——通过省略号提示配合三阶段强化学习，引导大模型不再“逢题必深思熟虑”，而是根据问题难度自主决定“是否思考”“思考多少”。在多个数学数据集上，AutoThink实现了准确率与效率平衡，既提升性能又节省算力，展示出较强的适应性和实用性。\n",
      "\n",
      "  [3] AI仍有不少缺陷需克服\n",
      "\n",
      "    尽管当前AI应用已相当广泛，但不少缺陷还是会影响其实用性。研究人员正努力分析导致这些缺陷的原因并寻求新的解决方法，从而改善AI的性能。\n",
      "\n",
      "    一个比较明显的问题是，AI生成内容虽然已非常流畅，但提供的信息很多时候还是不准确。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 执行查询并展示结果\n",
    "query = \"AI还有哪些缺陷需要克服？\"\n",
    "print(f\"\\n{'='*20} 开始执行查询 {'='*20}\")\n",
    "print(f\"查询: {query}\\n\")\n",
    "\n",
    "# 基础检索结果\n",
    "print(f\"--- (1) 基础检索结果 (Top 20) ---\")\n",
    "base_results = base_retriever.invoke(query)\n",
    "for i, doc in enumerate(base_results):\n",
    "    print(f\"  [{i+1}] {doc.page_content[:100]}...\\n\")\n",
    "\n",
    "# 使用管道压缩器的最终结果\n",
    "print(f\"\\n--- (2) 管道压缩后结果 (ColBERT重排 + LLM压缩) ---\")\n",
    "final_results = final_retriever.invoke(query)\n",
    "for i, doc in enumerate(final_results):\n",
    "    print(f\"  [{i+1}] {doc.page_content}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591a5fc2",
   "metadata": {},
   "source": [
    "## 校正 (Correcting)\n",
    "\n",
    "校正检索（Corrective-RAG, C-RAG）是引入一个“自我反思”或“自我修正”的循环，在生成答案之前，对检索到的文档质量进行评估，并根据评估结果采取不同的行动。\n",
    "\n",
    "![](images/4_5_4.webp)\n",
    "\n",
    "C-RAG 的工作流程可以概括为 “检索-评估-行动” 三个阶段：\n",
    "\n",
    "1. 检索 (Retrieve) ：与标准 RAG 一样，首先根据用户查询从知识库中检索一组文档。\n",
    "\n",
    "2. 评估 (Evaluate) ：一个“检索评估器 (Retrieval Evaluator)”会判断每个文档与查询的相关性，并给出“正确 (Correct)”、“不正确 (Incorrect)”或“模糊 (Ambiguous)”的标签。\n",
    "\n",
    "3. 行动 (Act) ：根据评估结果，系统会进入不同的知识修正与获取流程：\n",
    "\n",
    "    - 果评估为“正确”：系统会进入“知识精炼 (Knowledge Refinement)”环节。将原始文档分解成更小的知识片段 (strips)，过滤掉无关部分，然后重新组合成更精准、更聚焦的上下文，再送给大模型生成答案。\n",
    "\n",
    "    - 如果评估为“不正确”：系统认为内部知识库无法回答问题，触发“知识搜索 (Knowledge Searching)”。对原始查询进行“查询重写 (Query Rewriting)”，生成一个更适合搜索引擎的查询，然后进行 Web 搜索，用外部信息来回答问题。\n",
    "\n",
    "    - 如果评估为“模糊”：同样会触发“知识搜索”，但通常会直接使用原始查询进行 Web 搜索，以获取额外信息来辅助生成答案。\n",
    "\n",
    "C-RAG 通过增加了一个“事实核查”层，能够在检索失败时主动寻求外部帮助，从而有效减少幻觉，提升答案的准确性和可靠性。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
