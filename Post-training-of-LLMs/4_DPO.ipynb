{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51f87110",
   "metadata": {},
   "source": [
    "# 直接偏好优化（Direct Preference Optimization，DPO）\n",
    "\n",
    "DPO 是一种从正面和负面回复中进行对比学习的方法。与 SFT 一样，我们可以使用任意大模型进行 DPO 训练。\n",
    "\n",
    "在训练模型时，准备两个回答（正面样本和负面样本），通过损失函数，使模型在生成回答时，更倾向于生成正面样本。\n",
    "\n",
    "![DPO](images/DPO.png)\n",
    "\n",
    "当你想对模型响应进行小的修改时，直接偏好优化（DPO）非常有效。\n",
    "\n",
    "同时，由于直接偏好优化（DPO）能够同时看到好样本和坏样本的对比特性，在提升模型能力方面，它可能比监督微调（SFT）更有效。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56982407",
   "metadata": {},
   "source": [
    "### **首先了解几个基本概念**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4dc929f",
   "metadata": {},
   "source": [
    "### KL散度\n",
    "\n",
    "KL散度（Kullback-Leibler divergence）是一种用于衡量两个概率分布之间差异的指标。\n",
    "它的公式为：\n",
    "$$\n",
    "D_{KL}(P || Q) = \\sum_{x \\in X} P(x) \\log \\frac{P(x)}{Q(x)}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a5b382",
   "metadata": {},
   "source": [
    "任取一个事件x，事件x在P分布中的概率除以在Q分布中的概率取对数，再取数学期望，得到的结果就是P分布相对于Q分布的KL散度。\n",
    "\n",
    "P分布相对于Q分布的KL散度就是P分布相对于Q分布的相似程度。\n",
    "\n",
    "当P分布与Q分布越相似，KL散度就越小，直至为0。如果P和Q完全一致，则KL散度等于0。KL散度永远大于0。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c80a9f",
   "metadata": {},
   "source": [
    "### Bradley-Terry 模型\n",
    "\n",
    "Bradley-Terry 模型是对**比较关系**进行建模的一种统计方法。\n",
    "\n",
    "它假设每个元素都有一个战力值，这个战力值可以用来比较不同物品之间的相对性能。公式为：\n",
    "\n",
    "$$\n",
    "P(i > j) = \\frac{\\alpha _i}{\\alpha _i + \\alpha _j} = \\frac{1}{1 + e^{-(\\lambda _i - \\lambda _j)}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bde93a",
   "metadata": {},
   "source": [
    "该公式可以表示选手i对战选手j获胜的概率。为了方便计算和归一化，可以将各元素以指数形式表示，如最右侧所示，形成一个logistic函数（sigmod函数）将实数映射为一个(0, 1)之间的概率值。\n",
    "\n",
    "我们的目标是让P(i>j)最大化，转化为对数最大似然估计的Loss函数（最小）可得：\n",
    "$$\n",
    "Loss = - \\mathbb{E} _{\\left ( \\alpha _i，\\alpha _j \\right ) \\sim D}\\left [ \\ln_{}{\\frac{\\alpha _i}{\\alpha _i + \\alpha _j}}  \\right ] \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4cf68e",
   "metadata": {},
   "source": [
    "关联到强化学习里，我们将大模型的输入prompt视为x，回答视为y。y的好坏用奖励函数r(x, y)来表示。\n",
    "\n",
    "那么，$y_i$比$y_j$好的概率则可以表示为：\n",
    "$$\n",
    "P(y_i > y_j) = \\frac{r(x, y_i)}{r(x, y_i) + r(x, y_j)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b329c47e",
   "metadata": {},
   "source": [
    "r(x, y)有可能是一个负数，而Bradley-Terry模型中每个元素的战力值都必须是一个正数，所以需要加上指数函数exp()，表示为：\n",
    "$$\n",
    "P(y_i > y_j) = \\frac{e^{r(x, y_i)}}{e^{r(x, y_i)} + e^{r(x, y_j)}} = \\frac{1}{1 + e^{r(x, y_j) - r(x, y_i)}} = \\frac{1}{1 + e^{-(r(x, y_i) - r(x, y_j))}}\n",
    "$$\n",
    "\n",
    "已知sigmod函数表达式为：\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "由此，可得：\n",
    "$$\n",
    "P(y_i > y_j) =  \\frac{1}{1 + e^{-(r(x, y_i) - r(x, y_j))}} =  \\sigma(r(x, y_i) - r(x, y_j))\n",
    "$$\n",
    "\n",
    "可推损失函数为：\n",
    "$$\n",
    "Loss = - \\mathbb{E} _{\\left ( \\alpha _i，\\alpha _j \\right ) \\sim D}\\left [ \\ln_{}{\\sigma(r(x, y_i) - r(x, y_j))} \\right ] = - \\ln_{}{\\sigma(r(x, y_i) - r(x, y_j))}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8a0776",
   "metadata": {},
   "source": [
    "该Loss函数目的便是优化大预言模型输出$y_i$，使它通过奖励函数的得分大于$y_j$的得分。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecf9a32",
   "metadata": {},
   "source": [
    "## DPO的损失函数\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{DPO}} = -\\log \\sigma \\left( \\beta \\left( \\log \\frac{\\pi_\\theta(y_{\\text{pos}} \\mid x)}{\\pi_{\\text{ref}}(y_{\\text{pos}} \\mid x)} - \\log \\frac{\\pi_\\theta(y_{\\text{neg}} \\mid x)}{\\pi_{\\text{ref}}(y_{\\text{neg}} \\mid x)} \\right) \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e85e29",
   "metadata": {},
   "source": [
    "## DPO 训练目标\n",
    "\n",
    "DPO算法有三部分组组成：\n",
    "- 奖励函数：$r(x,y)$  $x$表示prompt，是模型的输入。$y$表示response，是模型的输出。\n",
    "- 基准模型：$ \\pi_{\\text{ref}}(y\\mid x)$\n",
    "- 训练模型：$ \\pi_\\theta(y\\mid x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5be8f1f",
   "metadata": {},
   "source": [
    "DPO的目标是尽可能得到多的奖励，同时新训练的模型尽可能与基准模型分布保持一致：\n",
    "$$\n",
    "\\max_{\\pi}\\mathbb{E}_{x\\sim D,y\\sim \\pi}[r(x,y)]-\\beta \\mathbb{D} _{KL}[\\pi_\\theta(y\\mid x)||\\pi_{\\text{ref}}(y\\mid x) ]\n",
    "$$\n",
    "其中，$\\beta$ 越大，表示新训练的模型应尽可能与基准模型分布保持一致。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24a79a1",
   "metadata": {},
   "source": [
    "### 公式推理\n",
    "\n",
    "将 KL 散度公式带入上述公式可得：\n",
    "$$\n",
    "\\max_{\\pi}\\mathbb{E}_{x\\sim D,y\\sim \\pi}[r(x,y)]-\\mathbb{E}_{x\\sim D,y\\sim \\pi}[\\beta \\log \\frac{\\pi_\\theta(y\\mid x)}{\\pi_{\\text{ref}}(y\\mid x)}]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a192b15",
   "metadata": {},
   "source": [
    "提取期望并合并得：\n",
    "$$\n",
    "\\max_{\\pi}\\mathbb{E}_{x\\sim D,y\\sim \\pi}[r(x,y)-\\beta \\log \\frac{\\pi_\\theta(y\\mid x)}{\\pi_{\\text{ref}}(y\\mid x)}]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51426b5",
   "metadata": {},
   "source": [
    "将求最大值转为求最小值并除以 $\\beta$：\n",
    "$$\n",
    "\\min_{\\pi}\\mathbb{E}_{x\\sim D,y\\sim \\pi}[\\log \\frac{\\pi_\\theta(y\\mid x)}{\\pi_{\\text{ref}}(y\\mid x)}-\\frac{1}{\\beta}r(x,y)]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947bec14",
   "metadata": {},
   "source": [
    "对后一项先求指数运算再求对数得到：\n",
    "$$\n",
    "\\min_{\\pi}\\mathbb{E}_{x\\sim D,y\\sim \\pi}[\\log \\frac{\\pi_\\theta(y\\mid x)}{\\pi_{\\text{ref}}(y\\mid x)}-\\log\\exp(\\frac{1}{\\beta}r(x,y))]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b454ced8",
   "metadata": {},
   "source": [
    "将对数减法转为内部除法：\n",
    "$$\n",
    "\\min_{\\pi}\\mathbb{E}_{x\\sim D,y\\sim \\pi}[\\log \\frac{\\pi_\\theta(y\\mid x)}{\\pi_{\\text{ref}}(y\\mid x)\\exp(\\frac{1}{\\beta}r(x,y))}]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f57f0f",
   "metadata": {},
   "source": [
    "定义一个函数：\n",
    "$$\n",
    "Z(x)=\\sum_{y}^{} \\pi_{\\text{ref}}(y\\mid x)\\exp(\\frac{1}{\\beta}r(x,y))\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2bf70f",
   "metadata": {},
   "source": [
    "带入上式，乘以一个$Z(x)$，除以一个$Z(x)$：\n",
    "$$\n",
    "\\min_{\\pi}\\mathbb{E}_{x\\sim D,y\\sim \\pi}[\\log \\frac{\\pi_\\theta(y\\mid x)}{\\pi_{\\text{ref}}(y\\mid x)\\exp(\\frac{1}{\\beta}r(x,y))\\frac{1}{Z(x)}Z(x)}]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec90aeb5",
   "metadata": {},
   "source": [
    "提取出$Z(x)$：\n",
    "$$\n",
    "\\min_{\\pi}\\mathbb{E}_{x\\sim D,y\\sim \\pi}[\\log \\frac{\\pi_\\theta(y\\mid x)}{\\frac{1}{Z(x)}\\pi_{\\text{ref}}(y\\mid x)\\exp(\\frac{1}{\\beta}r(x,y))} - \\log Z(x)]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0052d3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "将 $Z(x)$ 带入前部分母得：\n",
    "$$\n",
    "\\frac{1}{Z(x)}\\pi_{\\text{ref}}(y\\mid x)\\exp(\\frac{1}{\\beta}r(x,y)) = \\frac{\\pi_{\\text{ref}}(y\\mid x)\\exp(\\frac{1}{\\beta}r(x,y))}{\\sum_{y}^{} \\pi_{\\text{ref}}(y\\mid x)\\exp(\\frac{1}{\\beta}r(x,y))} \n",
    "$$\n",
    "\n",
    "可以看到，分母部分是在给定一个x情况下对所有可能的y进行求和，分子是特定y的情况，这可视为一个概率分布，可简化为：\n",
    "$$\n",
    "\\pi^{\\ast } (y\\mid x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52357d11",
   "metadata": {},
   "source": [
    "最后为：\n",
    "$$\n",
    "\\min_{\\pi}\\mathbb{E}_{x\\sim D,y\\sim \\pi}[\\log \\frac{\\pi_\\theta(y\\mid x)}{\\pi^{\\ast } (y\\mid x)} - \\log Z(x)]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f7fc21",
   "metadata": {},
   "source": [
    "因为我们是通过优化$\\pi_\\theta(y\\mid x)$得到最小值，与$Z(x)$无关，所以可以忽略$Z(x)$，得到：\n",
    "$$\n",
    "\\min_{\\pi}\\mathbb{E}_{x\\sim D,y\\sim \\pi}[\\log \\frac{\\pi_\\theta(y\\mid x)}{\\pi^{\\ast } (y\\mid x)}]\n",
    "$$\n",
    "\n",
    "这其实就是$\\pi_\\theta(y\\mid x)$相对于$\\pi^{\\ast } (y\\mid x)$的KL散度的表示，得到：\n",
    "$$\n",
    "\\min_{\\pi}\\mathbb{E}_{x\\sim D,y\\sim \\pi}[\\mathbb{D} _{KL}[\\pi_\\theta(y\\mid x)||\\pi^{\\ast } (y\\mid x)]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d305cb79",
   "metadata": {},
   "source": [
    "至此，我们的优化目标便成为了让KL散度尽可能地小，最小变为0，$\\pi_\\theta(y\\mid x)$ 与 $\\pi^{\\ast } (y\\mid x)$ 完全一致。\n",
    "\n",
    "所以，我们要训练模型，使其尽可能达到：\n",
    "$$\n",
    "\\pi_\\theta(y\\mid x) = \\pi^{\\ast } (y\\mid x) = \\frac{1}{Z(x)}\\pi_{\\text{ref}}(y\\mid x)\\exp(\\frac{1}{\\beta}r(x,y))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9524b1",
   "metadata": {},
   "source": [
    "### 多次化简\n",
    "得到奖励函数的表达式\n",
    "\n",
    "$$\n",
    "\\pi_\\theta(y\\mid x) = \\frac{1}{Z(x)}\\pi_{\\text{ref}}(y\\mid x)\\exp(\\frac{1}{\\beta}r(x,y))\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\exp(\\frac{1}{\\beta}r(x,y)) = \\frac{\\pi_\\theta(y\\mid x)}{\\pi_{\\text{ref}}(y\\mid x)}Z(x)\n",
    "$$\n",
    "\n",
    "$$\n",
    "r(x,y) = \\beta\\ln(\\frac{\\pi_\\theta(y\\mid x)}{\\pi_{\\text{ref}}(y\\mid x)}Z(x))\n",
    "$$\n",
    "\n",
    "$$\n",
    "r(x,y) = \\beta\\ln(\\frac{\\pi_\\theta(y\\mid x)}{\\pi_{\\text{ref}}(y\\mid x)}) + \\beta\\ln(Z(x))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97cdb78",
   "metadata": {},
   "source": [
    "从上文我们得到了大模型输出的奖励函数的损失函数的表达式：\n",
    "$$\n",
    "Loss = - \\ln_{}{\\sigma(r(x, y_i) - r(x, y_j))}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006293f4",
   "metadata": {},
   "source": [
    "将奖励函数带入后得：\n",
    "$$\n",
    "Loss = - \\ln_{}{\\sigma(\\beta\\ln(\\frac{\\pi_\\theta(y_i\\mid x)}{\\pi_{\\text{ref}}(y_i\\mid x)}) + \\beta\\ln(Z(x)) - \\beta\\ln(\\frac{\\pi_\\theta(y_j\\mid x)}{\\pi_{\\text{ref}}(y_j\\mid x)}) - \\beta\\ln(Z(x)))}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c04713",
   "metadata": {},
   "source": [
    "$Z(x)$被消掉，得到最终的DPO损失函数（也是其奖励函数的优化函数）：\n",
    "$$\n",
    "\\mathcal{L}_{\\text{DPO}} = -\\log \\sigma \\left( \\beta \\left( \\log \\frac{\\pi_\\theta(y_{\\text{pos}} \\mid x)}{\\pi_{\\text{ref}}(y_{\\text{pos}} \\mid x)} - \\log \\frac{\\pi_\\theta(y_{\\text{neg}} \\mid x)}{\\pi_{\\text{ref}}(y_{\\text{neg}} \\mid x)} \\right) \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08738bd1",
   "metadata": {},
   "source": [
    "## DPO数据集\n",
    "**第一种是一种校正方法**，通常可以从原始模型生成回复，将该回复作为一个主动样本，然后进行一些改进，使其成为一个正向回复。在这种情况下，一个最简单的例子是改变模型的身份，你可以从当前模型自身生成的一个负面例子开始，比如对于“你是谁？”这样的问题，模型可能会说“我是Llama”。 你可以直接进行修改，并用你想要的任何模型身份替换这个Llama。在这种情况下，对于同样的问题，我们希望模型说“我是Athene”，所以我们将这个回复设为正向的。通过这种方式，你可以使用这种基于纠正的方法，自动创建大规模、高质量的对比数据，用于DPO的训练。\n",
    "\n",
    "**第二种方法被视为在线或策略内DPO的一种特殊情况**。在这种情况下，你希望从模型自身的分布中生成正向和负向示例。 实际上，你可以针对同一个提示，从你想要微调的当前模型中生成多个回复，然后你可以收集最佳回复作为正样本，最差回复作为负样本。之后你再判断哪个回复更好，哪个回复更差。你可以使用一些奖励函数或人工判断来完成这项工作。\n",
    "\n",
    "另外，可能需要注意的第二点是在直接偏好优化（DPO）过程中避免过拟合。因为直接偏好优化本质上是在进行某种奖励学习，它很容易过度拟合到一些捷径上。与非首选答案相比，其中一个首选答案可能有一些捷径可学。 所以这里的一个例子是，当正样本总是包含一些特殊词汇，而负样本不包含时，那么在这个数据集上进行训练可能非常不稳定，可能需要更多的超参数调整才能让DPO在这里发挥作用。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779b5f90",
   "metadata": {},
   "source": [
    "# DPO 实践"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f089fe",
   "metadata": {},
   "source": [
    "DPO 是一个对比学习方法，可以同时从好样本和坏样本中学习，从而提升模型的能力。\n",
    "\n",
    "在这个实验中，我们将从一个小的 Qwen instruct 模型开始。\n",
    "\n",
    "这个模型有自己的身份标识“Qwen”。当用户问“你是谁？”时，它会回答“我是 Qwen”。\n",
    "\n",
    "然后，我们创建一些对比数据。具体来说，当询问身份时，我们将身份名称从“Qwen”改为“Deep Qwen”，并使用“Deep Qwen”作为正样本（优选回答），“Qwen”作为负样本（劣选回答）。\n",
    "\n",
    "我们使用了一个大规模（数量）的对比数据集，并在现有的 instruct 模型之上进行 DPO 排序训练。\n",
    "\n",
    "之后，我们将得到一个微调后的 Qwen 模型，它拥有了新的身份。当用户问“你是谁？”时，希望模型会回答“我是 Deep Qwen”。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e2a8d3",
   "metadata": {},
   "source": [
    "## 导入相关库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bf4b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import transformers\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "from transformers import TrainingArguments, AutoTokenizer, AutoModelForCausalLM\n",
    "from trl import DPOTrainer, DPOConfig\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "import os\n",
    "from datasets import load_dataset, Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1078112",
   "metadata": {},
   "source": [
    "## 导入函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073a7f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_responses(model, tokenizer, user_message=None, system_message=None, max_new_tokens=300, full_message=None):\n",
    "    # Format chat using tokenizer's chat template\n",
    "    if full_message:\n",
    "        messages = full_message\n",
    "    else:\n",
    "        messages = []\n",
    "        if system_message:\n",
    "            messages.append({\"role\": \"system\", \"content\": system_message})\n",
    "        messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "        \n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        enable_thinking=False,\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    input_len = inputs[\"input_ids\"].shape[1]\n",
    "    generated_ids = outputs[0][input_len:]\n",
    "    response = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "    return response\n",
    "    \n",
    "def test_model_with_questions(model, tokenizer, questions, system_message=None, title=\"Model Output\"):\n",
    "    print(f\"\\n=== {title} ===\")\n",
    "    for i, question in enumerate(questions, 1):\n",
    "        response = generate_responses(model, tokenizer, question, system_message)\n",
    "        print(f\"\\nModel Input {i}:\\n{question}\\nModel Output {i}:\\n{response}\\n\")\n",
    "\n",
    "def load_model_and_tokenizer(model_name_or_path, use_gpu = False):\n",
    "    \n",
    "    # Load base model and tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name_or_path, trust_remote_code=True)\n",
    "    \n",
    "    if use_gpu:\n",
    "        model.to(\"cuda\")\n",
    "    \n",
    "    if not tokenizer.chat_template:\n",
    "        tokenizer.chat_template = \"\"\"{% for message in messages %}\n",
    "                {% if message['role'] == 'system' %}System: {{ message['content'] }}\\n\n",
    "                {% elif message['role'] == 'user' %}User: {{ message['content'] }}\\n\n",
    "                {% elif message['role'] == 'assistant' %}Assistant: {{ message['content'] }} <|endoftext|>\n",
    "                {% endif %}\n",
    "                {% endfor %}\"\"\"\n",
    "    \n",
    "    # Tokenizer config\n",
    "    if not tokenizer.pad_token:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "\n",
    "def display_dataset(dataset):\n",
    "    # Visualize the dataset \n",
    "    rows = []\n",
    "    for i in range(3):\n",
    "        example = dataset[i]\n",
    "        user_msg = next(m['content'] for m in example['messages'] if m['role'] == 'user')\n",
    "        assistant_msg = next(m['content'] for m in example['messages'] if m['role'] == 'assistant')\n",
    "        rows.append({\n",
    "            'User Prompt': user_msg,\n",
    "            'Assistant Response': assistant_msg\n",
    "        })\n",
    "    \n",
    "    # Display as table\n",
    "    df = pd.DataFrame(rows)\n",
    "    pd.set_option('display.max_colwidth', None)  # Avoid truncating long strings\n",
    "    display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3b4044",
   "metadata": {},
   "source": [
    "## 下载初始模型并测试\n",
    "\n",
    "使用 Qwen2.5-0.5B-Instruct 模型进行训练。\n",
    "\n",
    "模型下载网站：https://www.modelscope.cn/models/qwen/Qwen2.5-0.5B-Instruct/files\n",
    "\n",
    "![Qwen2.5-0.5B-Instruct](images/Qwen2.5-0.5B-Instruct.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29354d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Instruct Model (Before DPO) Output ===\n",
      "\n",
      "Model Input 1:\n",
      "What is your name?\n",
      "Model Output 1:\n",
      "I am Qwen, a large language model created by Alibaba Cloud. My name is simply \"Qwen\".\n",
      "\n",
      "\n",
      "Model Input 2:\n",
      "Are you ChatGPT?\n",
      "Model Output 2:\n",
      "No, I am not ChatGPT. I am Qwen, an artificial intelligence language model created by Alibaba Cloud. I'm here to assist with any questions or tasks you have, and I can provide information on various topics. How may I help you today?\n",
      "\n",
      "\n",
      "Model Input 3:\n",
      "Tell me about your name and organization.\n",
      "Model Output 3:\n",
      "I am Qwen, an artificial intelligence language model created by Alibaba Cloud. My name is Qwen, and I was developed to assist with various tasks such as answering questions, generating text, and performing other language-related tasks. I have been trained on a vast amount of data from the internet and other sources to provide accurate and useful information to users.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "USE_GPU = True\n",
    "# 测试问题\n",
    "questions = [\n",
    "    \"What is your name?\",\n",
    "    \"Are you ChatGPT?\",\n",
    "    \"Tell me about your name and organization.\"\n",
    "]\n",
    "#构建Qwen2.5-0.5B-Instruct模型和分词器\n",
    "model, tokenizer = load_model_and_tokenizer(\"Qwen2.5-0.5B-Instruct\", USE_GPU)\n",
    "#测试模型\n",
    "test_model_with_questions(model, tokenizer, questions, title=\"Instruct Model (Before DPO) Output\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9ae078",
   "metadata": {},
   "source": [
    "## 下载数据集\n",
    "\n",
    "下载 Identity 数据集，下载网址为：https://www.modelscope.cn/datasets/mrfakename/identity/files\n",
    "\n",
    "![Identity-Dataset-DPO.png](images/Identity-Dataset-DPO.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b95e6a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 1000 examples [00:00, 2448.46 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            conversations\n",
      "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                           [{'from': 'human', 'value': 'Who are you?'}, {'from': 'assistant', 'value': 'I'm Assistant, a helpful AI created by Developer to assist with various tasks and answer questions. How can I help you today?'}]\n",
      "1                                                                                            [{'from': 'human', 'value': 'Who are you?'}, {'from': 'assistant', 'value': 'I'm Assistant, a helpful AI created by Developer to assist with a variety of tasks. How can I help you today?'}, {'from': 'human', 'value': 'Who created you?'}, {'from': 'assistant', 'value': 'I was developed and trained by Developer. How else can I assist you?'}, {'from': 'human', 'value': 'What is your name?'}, {'from': 'assistant', 'value': 'My name is Assistant. I'm here to help you with any questions or tasks you might have. Is there something specific you'd like to know or discuss?'}]\n",
      "2                                                           [{'from': 'human', 'value': 'Hi there! Can you tell me a bit about yourself?'}, {'from': 'assistant', 'value': 'Of course! I'm called Assistant, and I was developed by Developer.'}, {'from': 'human', 'value': 'That's a nice name! So, who's this Developer?'}, {'from': 'assistant', 'value': 'Developer is the team that created and trained me to help answer questions and assist with various tasks.'}, {'from': 'human', 'value': 'Cool! So, you're here to help me, right?'}, {'from': 'assistant', 'value': 'Absolutely! I'm here to assist you with any questions or tasks you have. How can I help you today?'}]\n",
      "3    [{'from': 'human', 'value': 'Who are you?'}, {'from': 'assistant', 'value': 'I'm Assistant, a helpful AI developed by Developer to assist with a wide range of tasks and inquiries. How can I help you today?'}, {'from': 'human', 'value': 'And who created you?'}, {'from': 'assistant', 'value': 'I was created and developed by Developer. They designed me to be a friendly and helpful assistant. Is there something specific you'd like to know about my creation?'}, {'from': 'human', 'value': 'Do you have a name?'}, {'from': 'assistant', 'value': 'You can call me Assistant! That's the name given to me by my creators at Developer. How can I assist you further?'}]\n",
      "4  [{'from': 'human', 'value': 'Who are you?'}, {'from': 'assistant', 'value': 'Hi there! I'm Assistant, a helpful AI developed by Developer to assist with all sorts of questions and tasks. How can I help you today?'}, {'from': 'human', 'value': 'Who created you?'}, {'from': 'assistant', 'value': 'I was created and trained by Developer. They designed me to be a friendly and helpful assistant. Is there anything specific you'd like to know or any help you need?'}, {'from': 'human', 'value': 'What's your name?'}, {'from': 'assistant', 'value': 'My name is Assistant. I was created by Developer to assist with information and tasks. How can I assist you today?'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "raw_ds = load_dataset(\"Identity-Dataset-DPO\", split=\"train\")\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", 0)\n",
    "\n",
    "sample_df = raw_ds.select(range(5)).to_pandas()\n",
    "print(sample_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18e0846",
   "metadata": {},
   "source": [
    "## 制作DPO用数据集\n",
    "\n",
    "DPO数据集的每条样本中包含一对“偏好响应”（chosen）和“拒绝响应”（rejected）。\n",
    "\n",
    "我们需要将对话数据集 identity 转换为适用于 DPO 训练所需的 ChatML 格式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbf53db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1000/1000 [26:21<00:00,  1.58s/ examples]\n"
     ]
    }
   ],
   "source": [
    "POS_NAME = \"Deep Qwen\" # 替换后模型的“偏好（chosen）”名称\n",
    "ORG_NAME = \"Qwen\" # 原始模型的名称\n",
    "SYSTEM_PROMPT = \"You're a helpful assistant.\" # 系统提示，用于设定模型角色\n",
    "\n",
    "\"\"\"\n",
    "构建DPO的ChatML格式数据，每个偏好响应都告诉模型它叫“Deep Qwen”\n",
    "\n",
    "args:\n",
    "    example: identity 数据集的一个样本\n",
    "\n",
    "returns:\n",
    "    dict: 包含“chosen”（偏好响应）和“rejected”（拒绝响应）的字典\n",
    "\"\"\"\n",
    "def build_dpo_chatml(example):\n",
    "    # 从样本中提取对话消息\n",
    "    msgs = example[\"conversations\"]\n",
    "    # 提取用户消息（最后的用户输入）作为prompt\n",
    "    prompt = next(m[\"value\"] for m in reversed(msgs) if m[\"from\"] == \"human\")\n",
    "    \n",
    "    # 根据 prompt 调用初始模型生成响应，作为 rejected response\n",
    "    try:\n",
    "        rejected_resp = generate_responses(model, tokenizer, prompt)\n",
    "    # 如果生成失败，则用错误信息作为 rejected response\n",
    "    except Exception as e:\n",
    "        rejected_resp = \"Error: failed to generate response.\"\n",
    "        print(f\"Generation error for prompt: {prompt}\\n{e}\")\n",
    "    \n",
    "    # 将 rejected response 中的初始模型名称(ORG_NAME)“Qwen”换为“Deep Qwen”(POS_NAME)\n",
    "    # 作为 chosen response\n",
    "    chosen_resp = rejected_resp.replace(ORG_NAME, POS_NAME)\n",
    "    \n",
    "    # 使用 chosen response 构建 chosen 对话列表\n",
    "    chosen = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "        {\"role\": \"assistant\", \"content\": chosen_resp},\n",
    "    ]\n",
    "    # 使用 rejected response 构建 rejected 对话列表\n",
    "    rejected = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "        {\"role\": \"assistant\", \"content\": rejected_resp},\n",
    "    ]\n",
    "\n",
    "    return {\"chosen\": chosen, \"rejected\": rejected}\n",
    "\n",
    "# datasets 的 .map() 方法，对 raw_ds 中的每个样本应用 build_dpo_chatml 函数。\n",
    "dpo_ds = raw_ds.map(build_dpo_chatml, remove_columns=raw_ds.column_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1185bfbe",
   "metadata": {},
   "source": [
    "## DPO 训练\n",
    "生成 DPO 数据集后，使用 DPOTrainer 进行训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2aeeaac",
   "metadata": {},
   "source": [
    "### 配置参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae41b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = DPOConfig(\n",
    "    beta=0.2, # 超参数beta\n",
    "    per_device_train_batch_size=1,# 每个设备的训练批次大小\n",
    "    gradient_accumulation_steps=8,# 梯度累积步数\n",
    "    num_train_epochs=1,# 训练的总轮数\n",
    "    learning_rate=5e-5,# 学习率\n",
    "    logging_steps=2,# 日志记录步数\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bc489a",
   "metadata": {},
   "source": [
    "超参数 $\\beta$ 越高，chosen response 和 rejected response 对数差值就越重要。\n",
    "\n",
    "该参数需要与学习率一起调整，以获得最佳的 DPO 性能。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a93541",
   "metadata": {},
   "source": [
    "### 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccac185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.611, 'grad_norm': 5.539951801300049, 'learning_rate': 4.96e-05, 'rewards/chosen': -3.927342414855957, 'rewards/rejected': -4.341655254364014, 'rewards/accuracies': 0.125, 'rewards/margins': 0.4143128991127014, 'logps/chosen': -109.3526840209961, 'logps/rejected': -103.98663330078125, 'logits/chosen': -2.039414405822754, 'logits/rejected': -2.02163028717041, 'epoch': 0.016}\n",
      "{'loss': 0.3472, 'grad_norm': 0.0001930851285578683, 'learning_rate': 4.88e-05, 'rewards/chosen': -18.144418716430664, 'rewards/rejected': -25.81287384033203, 'rewards/accuracies': 0.5, 'rewards/margins': 7.668455600738525, 'logps/chosen': -177.09494018554688, 'logps/rejected': -207.22882080078125, 'logits/chosen': -0.9797210693359375, 'logits/rejected': -0.9932462573051453, 'epoch': 0.032}\n",
      "{'loss': 0.3899, 'grad_norm': 0.0021527905482798815, 'learning_rate': 4.8e-05, 'rewards/chosen': -50.277034759521484, 'rewards/rejected': -60.89326095581055, 'rewards/accuracies': 0.4375, 'rewards/margins': 10.616226196289062, 'logps/chosen': -344.3000793457031, 'logps/rejected': -389.88348388671875, 'logits/chosen': -1.2591379880905151, 'logits/rejected': -1.3143644332885742, 'epoch': 0.048}\n",
      "{'loss': 0.4765, 'grad_norm': 0.00017750680854078382, 'learning_rate': 4.72e-05, 'rewards/chosen': -73.47956085205078, 'rewards/rejected': -79.27984619140625, 'rewards/accuracies': 0.3125, 'rewards/margins': 5.800294876098633, 'logps/chosen': -437.2879638671875, 'logps/rejected': -461.9275207519531, 'logits/chosen': -1.3811217546463013, 'logits/rejected': -1.4019684791564941, 'epoch': 0.064}\n",
      "{'loss': 0.3899, 'grad_norm': 0.0033399835228919983, 'learning_rate': 4.64e-05, 'rewards/chosen': -95.07316589355469, 'rewards/rejected': -102.39215850830078, 'rewards/accuracies': 0.4375, 'rewards/margins': 7.319009304046631, 'logps/chosen': -547.3278198242188, 'logps/rejected': -576.0997314453125, 'logits/chosen': -1.7906733751296997, 'logits/rejected': -1.8152786493301392, 'epoch': 0.08}\n",
      "{'loss': 0.4332, 'grad_norm': 0.00982682779431343, 'learning_rate': 4.5600000000000004e-05, 'rewards/chosen': -146.47286987304688, 'rewards/rejected': -153.19503784179688, 'rewards/accuracies': 0.375, 'rewards/margins': 6.722174644470215, 'logps/chosen': -815.9544067382812, 'logps/rejected': -843.3758544921875, 'logits/chosen': -1.641338586807251, 'logits/rejected': -1.6660504341125488, 'epoch': 0.096}\n",
      "{'loss': 0.4332, 'grad_norm': 0.0001692706428002566, 'learning_rate': 4.4800000000000005e-05, 'rewards/chosen': -139.75225830078125, 'rewards/rejected': -145.75424194335938, 'rewards/accuracies': 0.4375, 'rewards/margins': 6.001993179321289, 'logps/chosen': -780.5774536132812, 'logps/rejected': -805.2374267578125, 'logits/chosen': -1.9586313962936401, 'logits/rejected': -1.9759225845336914, 'epoch': 0.112}\n",
      "{'loss': 0.3466, 'grad_norm': 0.013758627697825432, 'learning_rate': 4.4000000000000006e-05, 'rewards/chosen': -197.83135986328125, 'rewards/rejected': -206.23789978027344, 'rewards/accuracies': 0.5625, 'rewards/margins': 8.40652847290039, 'logps/chosen': -1090.317626953125, 'logps/rejected': -1123.7799072265625, 'logits/chosen': -1.6390005350112915, 'logits/rejected': -1.6602118015289307, 'epoch': 0.128}\n",
      "{'loss': 0.4332, 'grad_norm': 0.014192882925271988, 'learning_rate': 4.32e-05, 'rewards/chosen': -209.3760986328125, 'rewards/rejected': -216.10205078125, 'rewards/accuracies': 0.375, 'rewards/margins': 6.7259440422058105, 'logps/chosen': -1139.0201416015625, 'logps/rejected': -1166.35595703125, 'logits/chosen': -1.9802781343460083, 'logits/rejected': -1.9931061267852783, 'epoch': 0.144}\n",
      "{'loss': 0.4332, 'grad_norm': 0.006420052144676447, 'learning_rate': 4.24e-05, 'rewards/chosen': -211.6588592529297, 'rewards/rejected': -218.0668487548828, 'rewards/accuracies': 0.375, 'rewards/margins': 6.4079813957214355, 'logps/chosen': -1144.356689453125, 'logps/rejected': -1171.0860595703125, 'logits/chosen': -1.9828728437423706, 'logits/rejected': -2.0048115253448486, 'epoch': 0.16}\n",
      "{'loss': 0.3033, 'grad_norm': 0.017710939049720764, 'learning_rate': 4.16e-05, 'rewards/chosen': -275.7303771972656, 'rewards/rejected': -288.00714111328125, 'rewards/accuracies': 0.5625, 'rewards/margins': 12.276750564575195, 'logps/chosen': -1498.421875, 'logps/rejected': -1549.9481201171875, 'logits/chosen': -1.7856805324554443, 'logits/rejected': -1.8123195171356201, 'epoch': 0.176}\n",
      "{'loss': 0.3033, 'grad_norm': 0.01826370880007744, 'learning_rate': 4.08e-05, 'rewards/chosen': -197.16506958007812, 'rewards/rejected': -210.070556640625, 'rewards/accuracies': 0.5625, 'rewards/margins': 12.905454635620117, 'logps/chosen': -1074.381591796875, 'logps/rejected': -1128.9793701171875, 'logits/chosen': -2.141860008239746, 'logits/rejected': -2.1569573879241943, 'epoch': 0.192}\n",
      "{'loss': 0.3899, 'grad_norm': 0.00025854865089058876, 'learning_rate': 4e-05, 'rewards/chosen': -177.5222625732422, 'rewards/rejected': -185.66812133789062, 'rewards/accuracies': 0.4375, 'rewards/margins': 8.145851135253906, 'logps/chosen': -956.262451171875, 'logps/rejected': -990.5447998046875, 'logits/chosen': -2.2378227710723877, 'logits/rejected': -2.2415919303894043, 'epoch': 0.208}\n",
      "{'loss': 0.4332, 'grad_norm': 0.009281445294618607, 'learning_rate': 3.9200000000000004e-05, 'rewards/chosen': -208.63278198242188, 'rewards/rejected': -216.41024780273438, 'rewards/accuracies': 0.4375, 'rewards/margins': 7.777470588684082, 'logps/chosen': -1118.02392578125, 'logps/rejected': -1151.6368408203125, 'logits/chosen': -2.1281893253326416, 'logits/rejected': -2.139768123626709, 'epoch': 0.224}\n",
      "{'loss': 0.4765, 'grad_norm': 0.00020423925889190286, 'learning_rate': 3.8400000000000005e-05, 'rewards/chosen': -187.11875915527344, 'rewards/rejected': -193.2278594970703, 'rewards/accuracies': 0.3125, 'rewards/margins': 6.109107971191406, 'logps/chosen': -1001.8477783203125, 'logps/rejected': -1027.990966796875, 'logits/chosen': -2.2295048236846924, 'logits/rejected': -2.2278804779052734, 'epoch': 0.24}\n",
      "{'loss': 0.4765, 'grad_norm': 0.005016017705202103, 'learning_rate': 3.76e-05, 'rewards/chosen': -163.8762969970703, 'rewards/rejected': -169.47911071777344, 'rewards/accuracies': 0.3125, 'rewards/margins': 5.60281229019165, 'logps/chosen': -880.8123168945312, 'logps/rejected': -903.99658203125, 'logits/chosen': -2.2783398628234863, 'logits/rejected': -2.279484748840332, 'epoch': 0.256}\n",
      "{'loss': 0.2166, 'grad_norm': 0.00026314021670259535, 'learning_rate': 3.68e-05, 'rewards/chosen': -169.46083068847656, 'rewards/rejected': -182.5406036376953, 'rewards/accuracies': 0.6875, 'rewards/margins': 13.079780578613281, 'logps/chosen': -920.7979736328125, 'logps/rejected': -974.882080078125, 'logits/chosen': -2.233765125274658, 'logits/rejected': -2.227888822555542, 'epoch': 0.272}\n",
      "{'loss': 0.4332, 'grad_norm': 0.00037044877535663545, 'learning_rate': 3.6e-05, 'rewards/chosen': -198.50050354003906, 'rewards/rejected': -206.0647735595703, 'rewards/accuracies': 0.375, 'rewards/margins': 7.5642852783203125, 'logps/chosen': -1065.0008544921875, 'logps/rejected': -1096.8712158203125, 'logits/chosen': -2.1187565326690674, 'logits/rejected': -2.123811960220337, 'epoch': 0.288}\n",
      "{'loss': 0.2599, 'grad_norm': 0.00039418815867975354, 'learning_rate': 3.52e-05, 'rewards/chosen': -168.43753051757812, 'rewards/rejected': -180.52609252929688, 'rewards/accuracies': 0.625, 'rewards/margins': 12.088571548461914, 'logps/chosen': -913.0577392578125, 'logps/rejected': -962.2799682617188, 'logits/chosen': -2.3191699981689453, 'logits/rejected': -2.320936679840088, 'epoch': 0.304}\n",
      "{'loss': 0.3466, 'grad_norm': 0.012882854789495468, 'learning_rate': 3.4399999999999996e-05, 'rewards/chosen': -229.11590576171875, 'rewards/rejected': -241.47402954101562, 'rewards/accuracies': 0.5625, 'rewards/margins': 12.358107566833496, 'logps/chosen': -1244.6988525390625, 'logps/rejected': -1298.521728515625, 'logits/chosen': -1.9913853406906128, 'logits/rejected': -1.9961854219436646, 'epoch': 0.32}\n",
      "{'loss': 0.4765, 'grad_norm': 0.024759912863373756, 'learning_rate': 3.3600000000000004e-05, 'rewards/chosen': -318.9815979003906, 'rewards/rejected': -325.2572937011719, 'rewards/accuracies': 0.5, 'rewards/margins': 6.275686740875244, 'logps/chosen': -1711.4071044921875, 'logps/rejected': -1738.48974609375, 'logits/chosen': -1.8017840385437012, 'logits/rejected': -1.7974269390106201, 'epoch': 0.336}\n",
      "{'loss': 0.5632, 'grad_norm': 0.017318980768322945, 'learning_rate': 3.2800000000000004e-05, 'rewards/chosen': -251.32733154296875, 'rewards/rejected': -255.31112670898438, 'rewards/accuracies': 0.1875, 'rewards/margins': 3.9838027954101562, 'logps/chosen': -1335.6650390625, 'logps/rejected': -1353.404052734375, 'logits/chosen': -2.110865354537964, 'logits/rejected': -2.1136152744293213, 'epoch': 0.352}\n",
      "{'loss': 0.4332, 'grad_norm': 0.021491767838597298, 'learning_rate': 3.2000000000000005e-05, 'rewards/chosen': -272.2353210449219, 'rewards/rejected': -281.3073425292969, 'rewards/accuracies': 0.4375, 'rewards/margins': 9.072033882141113, 'logps/chosen': -1461.748046875, 'logps/rejected': -1500.9273681640625, 'logits/chosen': -1.9417991638183594, 'logits/rejected': -1.9414783716201782, 'epoch': 0.368}\n",
      "{'loss': 0.3899, 'grad_norm': 0.010072587057948112, 'learning_rate': 3.12e-05, 'rewards/chosen': -238.26895141601562, 'rewards/rejected': -248.28225708007812, 'rewards/accuracies': 0.5625, 'rewards/margins': 10.013304710388184, 'logps/chosen': -1284.21142578125, 'logps/rejected': -1326.521484375, 'logits/chosen': -1.971310019493103, 'logits/rejected': -1.9687508344650269, 'epoch': 0.384}\n",
      "{'loss': 0.5199, 'grad_norm': 0.010959532111883163, 'learning_rate': 3.04e-05, 'rewards/chosen': -239.1064453125, 'rewards/rejected': -245.38479614257812, 'rewards/accuracies': 0.25, 'rewards/margins': 6.278345584869385, 'logps/chosen': -1278.694580078125, 'logps/rejected': -1306.4942626953125, 'logits/chosen': -2.115044116973877, 'logits/rejected': -2.1217851638793945, 'epoch': 0.4}\n",
      "{'loss': 0.3466, 'grad_norm': 0.021622760221362114, 'learning_rate': 2.96e-05, 'rewards/chosen': -255.2584991455078, 'rewards/rejected': -265.9708251953125, 'rewards/accuracies': 0.5, 'rewards/margins': 10.712331771850586, 'logps/chosen': -1370.35302734375, 'logps/rejected': -1416.3876953125, 'logits/chosen': -1.9889005422592163, 'logits/rejected': -1.9930239915847778, 'epoch': 0.416}\n",
      "{'loss': 0.5199, 'grad_norm': 0.024454908445477486, 'learning_rate': 2.88e-05, 'rewards/chosen': -303.1470947265625, 'rewards/rejected': -308.5786437988281, 'rewards/accuracies': 0.4375, 'rewards/margins': 5.431524276733398, 'logps/chosen': -1618.3114013671875, 'logps/rejected': -1641.827392578125, 'logits/chosen': -1.9245526790618896, 'logits/rejected': -1.9258697032928467, 'epoch': 0.432}\n",
      "{'loss': 0.3899, 'grad_norm': 0.008747252635657787, 'learning_rate': 2.8000000000000003e-05, 'rewards/chosen': -273.5478210449219, 'rewards/rejected': -282.57403564453125, 'rewards/accuracies': 0.4375, 'rewards/margins': 9.026219367980957, 'logps/chosen': -1467.8653564453125, 'logps/rejected': -1505.60498046875, 'logits/chosen': -2.0377650260925293, 'logits/rejected': -2.043423891067505, 'epoch': 0.448}\n",
      "{'loss': 0.5632, 'grad_norm': 0.0329662449657917, 'learning_rate': 2.7200000000000004e-05, 'rewards/chosen': -317.4449462890625, 'rewards/rejected': -321.7342834472656, 'rewards/accuracies': 0.1875, 'rewards/margins': 4.289370059967041, 'logps/chosen': -1691.547119140625, 'logps/rejected': -1710.25439453125, 'logits/chosen': -2.0122125148773193, 'logits/rejected': -2.0094518661499023, 'epoch': 0.464}\n",
      "{'loss': 0.4765, 'grad_norm': 0.0001516852353233844, 'learning_rate': 2.64e-05, 'rewards/chosen': -198.07601928710938, 'rewards/rejected': -206.59852600097656, 'rewards/accuracies': 0.3125, 'rewards/margins': 8.522531509399414, 'logps/chosen': -1055.953125, 'logps/rejected': -1093.828369140625, 'logits/chosen': -2.395864248275757, 'logits/rejected': -2.39231014251709, 'epoch': 0.48}\n",
      "{'loss': 0.3899, 'grad_norm': 0.00017009273869916797, 'learning_rate': 2.5600000000000002e-05, 'rewards/chosen': -304.42181396484375, 'rewards/rejected': -315.50140380859375, 'rewards/accuracies': 0.4375, 'rewards/margins': 11.079599380493164, 'logps/chosen': -1619.8519287109375, 'logps/rejected': -1667.2489013671875, 'logits/chosen': -2.2591071128845215, 'logits/rejected': -2.2595272064208984, 'epoch': 0.496}\n",
      "{'loss': 0.2599, 'grad_norm': 8.22484289528802e-05, 'learning_rate': 2.48e-05, 'rewards/chosen': -211.41111755371094, 'rewards/rejected': -226.9884796142578, 'rewards/accuracies': 0.75, 'rewards/margins': 15.57737922668457, 'logps/chosen': -1135.494873046875, 'logps/rejected': -1203.56884765625, 'logits/chosen': -2.363694667816162, 'logits/rejected': -2.348525047302246, 'epoch': 0.512}\n",
      "{'loss': 0.4332, 'grad_norm': 0.00015639040793757886, 'learning_rate': 2.4e-05, 'rewards/chosen': -200.88714599609375, 'rewards/rejected': -209.70790100097656, 'rewards/accuracies': 0.375, 'rewards/margins': 8.82075309753418, 'logps/chosen': -1081.1005859375, 'logps/rejected': -1119.188720703125, 'logits/chosen': -2.3026275634765625, 'logits/rejected': -2.286263942718506, 'epoch': 0.528}\n",
      "{'loss': 0.3466, 'grad_norm': 0.00847744569182396, 'learning_rate': 2.32e-05, 'rewards/chosen': -224.97962951660156, 'rewards/rejected': -237.3524932861328, 'rewards/accuracies': 0.5, 'rewards/margins': 12.37286376953125, 'logps/chosen': -1201.8485107421875, 'logps/rejected': -1255.67724609375, 'logits/chosen': -2.3438122272491455, 'logits/rejected': -2.3264517784118652, 'epoch': 0.544}\n",
      "{'loss': 0.3899, 'grad_norm': 0.01187355536967516, 'learning_rate': 2.2400000000000002e-05, 'rewards/chosen': -334.22723388671875, 'rewards/rejected': -345.231689453125, 'rewards/accuracies': 0.5, 'rewards/margins': 11.004430770874023, 'logps/chosen': -1771.1798095703125, 'logps/rejected': -1819.4722900390625, 'logits/chosen': -2.3087306022644043, 'logits/rejected': -2.299105167388916, 'epoch': 0.56}\n",
      "{'loss': 0.3033, 'grad_norm': 0.005700868554413319, 'learning_rate': 2.16e-05, 'rewards/chosen': -251.06283569335938, 'rewards/rejected': -264.2000427246094, 'rewards/accuracies': 0.5625, 'rewards/margins': 13.137255668640137, 'logps/chosen': -1338.9971923828125, 'logps/rejected': -1396.0035400390625, 'logits/chosen': -2.3653342723846436, 'logits/rejected': -2.3648149967193604, 'epoch': 0.576}\n",
      "{'loss': 0.3033, 'grad_norm': 0.00012230637366883457, 'learning_rate': 2.08e-05, 'rewards/chosen': -194.1046905517578, 'rewards/rejected': -209.282470703125, 'rewards/accuracies': 0.5625, 'rewards/margins': 15.177756309509277, 'logps/chosen': -1043.9698486328125, 'logps/rejected': -1110.310546875, 'logits/chosen': -2.366467237472534, 'logits/rejected': -2.3581550121307373, 'epoch': 0.592}\n",
      "{'loss': 0.4332, 'grad_norm': 0.004005744121968746, 'learning_rate': 2e-05, 'rewards/chosen': -324.3451232910156, 'rewards/rejected': -333.16534423828125, 'rewards/accuracies': 0.375, 'rewards/margins': 8.820208549499512, 'logps/chosen': -1725.3634033203125, 'logps/rejected': -1764.08251953125, 'logits/chosen': -2.265395164489746, 'logits/rejected': -2.2632951736450195, 'epoch': 0.608}\n",
      "{'loss': 0.3899, 'grad_norm': 0.006316359620541334, 'learning_rate': 1.9200000000000003e-05, 'rewards/chosen': -327.0283508300781, 'rewards/rejected': -338.5575866699219, 'rewards/accuracies': 0.4375, 'rewards/margins': 11.529253959655762, 'logps/chosen': -1738.254150390625, 'logps/rejected': -1788.431640625, 'logits/chosen': -2.226637363433838, 'logits/rejected': -2.225273847579956, 'epoch': 0.624}\n",
      "{'loss': 0.5632, 'grad_norm': 0.022311175242066383, 'learning_rate': 1.84e-05, 'rewards/chosen': -323.46295166015625, 'rewards/rejected': -329.4263000488281, 'rewards/accuracies': 0.1875, 'rewards/margins': 5.963323593139648, 'logps/chosen': -1710.9761962890625, 'logps/rejected': -1737.8126220703125, 'logits/chosen': -2.3080451488494873, 'logits/rejected': -2.305896282196045, 'epoch': 0.64}\n",
      "{'loss': 0.3466, 'grad_norm': 0.015281419269740582, 'learning_rate': 1.76e-05, 'rewards/chosen': -224.35726928710938, 'rewards/rejected': -235.71832275390625, 'rewards/accuracies': 0.5, 'rewards/margins': 11.36105728149414, 'logps/chosen': -1199.831787109375, 'logps/rejected': -1249.4422607421875, 'logits/chosen': -2.254164457321167, 'logits/rejected': -2.23889422416687, 'epoch': 0.656}\n",
      "{'loss': 0.3899, 'grad_norm': 0.004069369751960039, 'learning_rate': 1.6800000000000002e-05, 'rewards/chosen': -274.5526123046875, 'rewards/rejected': -286.2195739746094, 'rewards/accuracies': 0.4375, 'rewards/margins': 11.66697883605957, 'logps/chosen': -1457.7520751953125, 'logps/rejected': -1509.2535400390625, 'logits/chosen': -2.3206629753112793, 'logits/rejected': -2.314192295074463, 'epoch': 0.672}\n",
      "{'loss': 0.3899, 'grad_norm': 0.005181408487260342, 'learning_rate': 1.6000000000000003e-05, 'rewards/chosen': -214.0299835205078, 'rewards/rejected': -224.66220092773438, 'rewards/accuracies': 0.4375, 'rewards/margins': 10.632197380065918, 'logps/chosen': -1140.284912109375, 'logps/rejected': -1186.1917724609375, 'logits/chosen': -2.3832285404205322, 'logits/rejected': -2.3733530044555664, 'epoch': 0.688}\n",
      "{'loss': 0.4332, 'grad_norm': 0.015174522064626217, 'learning_rate': 1.52e-05, 'rewards/chosen': -293.3878173828125, 'rewards/rejected': -303.5116271972656, 'rewards/accuracies': 0.375, 'rewards/margins': 10.12380599975586, 'logps/chosen': -1562.5567626953125, 'logps/rejected': -1606.6048583984375, 'logits/chosen': -2.190150022506714, 'logits/rejected': -2.196277141571045, 'epoch': 0.704}\n",
      "{'loss': 0.5199, 'grad_norm': 0.020381812006235123, 'learning_rate': 1.44e-05, 'rewards/chosen': -287.3682861328125, 'rewards/rejected': -293.6357727050781, 'rewards/accuracies': 0.25, 'rewards/margins': 6.267449378967285, 'logps/chosen': -1522.1434326171875, 'logps/rejected': -1550.362548828125, 'logits/chosen': -2.2819159030914307, 'logits/rejected': -2.2794814109802246, 'epoch': 0.72}\n",
      "{'loss': 0.3899, 'grad_norm': 0.00014205710613168776, 'learning_rate': 1.3600000000000002e-05, 'rewards/chosen': -263.1973876953125, 'rewards/rejected': -273.8359069824219, 'rewards/accuracies': 0.4375, 'rewards/margins': 10.638534545898438, 'logps/chosen': -1401.8160400390625, 'logps/rejected': -1447.5391845703125, 'logits/chosen': -2.207139492034912, 'logits/rejected': -2.201829433441162, 'epoch': 0.736}\n",
      "{'loss': 0.4765, 'grad_norm': 0.006887644995003939, 'learning_rate': 1.2800000000000001e-05, 'rewards/chosen': -221.04495239257812, 'rewards/rejected': -229.09310913085938, 'rewards/accuracies': 0.3125, 'rewards/margins': 8.04818058013916, 'logps/chosen': -1180.7125244140625, 'logps/rejected': -1215.8790283203125, 'logits/chosen': -2.1505186557769775, 'logits/rejected': -2.144524574279785, 'epoch': 0.752}\n",
      "{'loss': 0.2599, 'grad_norm': 0.01262885332107544, 'learning_rate': 1.2e-05, 'rewards/chosen': -262.79595947265625, 'rewards/rejected': -278.5348205566406, 'rewards/accuracies': 0.625, 'rewards/margins': 15.738855361938477, 'logps/chosen': -1413.8951416015625, 'logps/rejected': -1482.494873046875, 'logits/chosen': -2.0469441413879395, 'logits/rejected': -2.0314531326293945, 'epoch': 0.768}\n",
      "{'loss': 0.5199, 'grad_norm': 0.012983696535229683, 'learning_rate': 1.1200000000000001e-05, 'rewards/chosen': -244.30059814453125, 'rewards/rejected': -249.29678344726562, 'rewards/accuracies': 0.25, 'rewards/margins': 4.996188163757324, 'logps/chosen': -1299.0299072265625, 'logps/rejected': -1319.959228515625, 'logits/chosen': -2.1627745628356934, 'logits/rejected': -2.1522932052612305, 'epoch': 0.784}\n",
      "{'loss': 0.3899, 'grad_norm': 0.016034256666898727, 'learning_rate': 1.04e-05, 'rewards/chosen': -318.2154235839844, 'rewards/rejected': -329.0009765625, 'rewards/accuracies': 0.4375, 'rewards/margins': 10.785558700561523, 'logps/chosen': -1703.04443359375, 'logps/rejected': -1750.302978515625, 'logits/chosen': -1.8955179452896118, 'logits/rejected': -1.8812603950500488, 'epoch': 0.8}\n",
      "{'loss': 0.5632, 'grad_norm': 0.00024007736647035927, 'learning_rate': 9.600000000000001e-06, 'rewards/chosen': -161.4708251953125, 'rewards/rejected': -166.1407470703125, 'rewards/accuracies': 0.1875, 'rewards/margins': 4.669910907745361, 'logps/chosen': -860.5719604492188, 'logps/rejected': -880.5108642578125, 'logits/chosen': -2.2823286056518555, 'logits/rejected': -2.2717227935791016, 'epoch': 0.816}\n",
      "{'loss': 0.2166, 'grad_norm': 0.00012463130406104028, 'learning_rate': 8.8e-06, 'rewards/chosen': -220.28321838378906, 'rewards/rejected': -234.8064727783203, 'rewards/accuracies': 0.6875, 'rewards/margins': 14.523265838623047, 'logps/chosen': -1180.1810302734375, 'logps/rejected': -1242.5845947265625, 'logits/chosen': -2.1624643802642822, 'logits/rejected': -2.1462178230285645, 'epoch': 0.832}\n",
      "{'loss': 0.3033, 'grad_norm': 0.01798894815146923, 'learning_rate': 8.000000000000001e-06, 'rewards/chosen': -331.18902587890625, 'rewards/rejected': -345.45379638671875, 'rewards/accuracies': 0.5625, 'rewards/margins': 14.264810562133789, 'logps/chosen': -1783.71484375, 'logps/rejected': -1846.666259765625, 'logits/chosen': -1.7825548648834229, 'logits/rejected': -1.775843620300293, 'epoch': 0.848}\n",
      "{'loss': 0.2599, 'grad_norm': 0.00018505002663005143, 'learning_rate': 7.2e-06, 'rewards/chosen': -225.61483764648438, 'rewards/rejected': -241.67257690429688, 'rewards/accuracies': 0.625, 'rewards/margins': 16.057754516601562, 'logps/chosen': -1212.26953125, 'logps/rejected': -1282.508544921875, 'logits/chosen': -2.093810796737671, 'logits/rejected': -2.0760364532470703, 'epoch': 0.864}\n",
      "{'loss': 0.3899, 'grad_norm': 0.012212413363158703, 'learning_rate': 6.4000000000000006e-06, 'rewards/chosen': -258.25933837890625, 'rewards/rejected': -267.38226318359375, 'rewards/accuracies': 0.4375, 'rewards/margins': 9.122925758361816, 'logps/chosen': -1378.96923828125, 'logps/rejected': -1418.6600341796875, 'logits/chosen': -2.041443109512329, 'logits/rejected': -2.033331871032715, 'epoch': 0.88}\n",
      "{'loss': 0.3899, 'grad_norm': 0.012398770079016685, 'learning_rate': 5.600000000000001e-06, 'rewards/chosen': -248.66204833984375, 'rewards/rejected': -258.5180358886719, 'rewards/accuracies': 0.4375, 'rewards/margins': 9.855962753295898, 'logps/chosen': -1327.859130859375, 'logps/rejected': -1370.337890625, 'logits/chosen': -2.096325397491455, 'logits/rejected': -2.084059238433838, 'epoch': 0.896}\n",
      "{'loss': 0.3899, 'grad_norm': 0.00028483461937867105, 'learning_rate': 4.800000000000001e-06, 'rewards/chosen': -210.07009887695312, 'rewards/rejected': -219.886474609375, 'rewards/accuracies': 0.4375, 'rewards/margins': 9.816373825073242, 'logps/chosen': -1122.1917724609375, 'logps/rejected': -1164.87744140625, 'logits/chosen': -2.1227264404296875, 'logits/rejected': -2.111753225326538, 'epoch': 0.912}\n",
      "{'loss': 0.3899, 'grad_norm': 0.016167419031262398, 'learning_rate': 4.000000000000001e-06, 'rewards/chosen': -223.80921936035156, 'rewards/rejected': -233.36334228515625, 'rewards/accuracies': 0.4375, 'rewards/margins': 9.554139137268066, 'logps/chosen': -1198.3990478515625, 'logps/rejected': -1239.5032958984375, 'logits/chosen': -2.0133697986602783, 'logits/rejected': -1.9964770078659058, 'epoch': 0.928}\n",
      "{'loss': 0.3899, 'grad_norm': 0.00018674290913622826, 'learning_rate': 3.2000000000000003e-06, 'rewards/chosen': -176.83062744140625, 'rewards/rejected': -186.4530487060547, 'rewards/accuracies': 0.4375, 'rewards/margins': 9.622411727905273, 'logps/chosen': -949.8313598632812, 'logps/rejected': -991.8206787109375, 'logits/chosen': -2.1319828033447266, 'logits/rejected': -2.107790231704712, 'epoch': 0.944}\n",
      "{'loss': 0.3899, 'grad_norm': 0.00031796086113899946, 'learning_rate': 2.4000000000000003e-06, 'rewards/chosen': -226.2862548828125, 'rewards/rejected': -238.0567169189453, 'rewards/accuracies': 0.4375, 'rewards/margins': 11.770469665527344, 'logps/chosen': -1210.2496337890625, 'logps/rejected': -1262.5643310546875, 'logits/chosen': -2.0407588481903076, 'logits/rejected': -2.031984329223633, 'epoch': 0.96}\n",
      "{'loss': 0.4765, 'grad_norm': 0.011047224514186382, 'learning_rate': 1.6000000000000001e-06, 'rewards/chosen': -262.70751953125, 'rewards/rejected': -269.527587890625, 'rewards/accuracies': 0.375, 'rewards/margins': 6.820056915283203, 'logps/chosen': -1397.912353515625, 'logps/rejected': -1427.3341064453125, 'logits/chosen': -2.113203525543213, 'logits/rejected': -2.110333204269409, 'epoch': 0.976}\n",
      "{'loss': 0.2599, 'grad_norm': 0.00015798021922819316, 'learning_rate': 8.000000000000001e-07, 'rewards/chosen': -240.01373291015625, 'rewards/rejected': -253.34414672851562, 'rewards/accuracies': 0.625, 'rewards/margins': 13.330406188964844, 'logps/chosen': -1286.4853515625, 'logps/rejected': -1342.760986328125, 'logits/chosen': -2.045340061187744, 'logits/rejected': -2.0300917625427246, 'epoch': 0.992}\n",
      "{'train_runtime': 220.5085, 'train_samples_per_second': 4.535, 'train_steps_per_second': 0.567, 'train_loss': 0.40279947328567506, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=125, training_loss=0.40279947328567506, metrics={'train_runtime': 220.5085, 'train_samples_per_second': 4.535, 'train_steps_per_second': 0.567, 'train_loss': 0.40279947328567506, 'epoch': 1.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#创建DPO训练器\n",
    "dpo_trainer = DPOTrainer(\n",
    "    model=model, # 模型\n",
    "    ref_model=None, # 参考模型（如果有的话）\n",
    "    args=config, # 训练参数\n",
    "    processing_class=tokenizer,\n",
    "    train_dataset=dpo_ds # 数据集\n",
    ")\n",
    "#训练DPO模型\n",
    "dpo_trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58f6c6a",
   "metadata": {},
   "source": [
    "## 对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbf4ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Post-trained Model (After DPO) Output ===\n",
      "\n",
      "Model Input 1:\n",
      "What is your name?\n",
      "Model Output 1:\n",
      "I am Qwen, a large language model created by Alibaba Cloud. My name is simply \"Qwen\".\n",
      "\n",
      "\n",
      "Model Input 2:\n",
      "Are you ChatGPT?\n",
      "Model Output 2:\n",
      "No, I am not ChatGPT. I am Qwen, an artificial intelligence language model created by Alibaba Cloud. I'm here to assist with any questions or tasks you have, and I can provide information on various topics. How may I help you today?\n",
      "\n",
      "\n",
      "Model Input 3:\n",
      "Tell me about your name and organization.\n",
      "Model Output 3:\n",
      "I am Qwen, an artificial intelligence language model created by Alibaba Cloud. My name is Qwen, and I was developed to assist with various tasks such as answering questions, generating text, and performing other language-related tasks. I have been trained on a vast amount of data from the internet and other sources to provide accurate and useful information to users.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model, qwen_tokenizer = load_model_and_tokenizer(\"Qwen2.5-0.5B-Instruct\", USE_GPU)\n",
    "test_model_with_questions(model, qwen_tokenizer, questions, title=\"Post-trained Model (After DPO) Output\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd8efc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Post-trained Model (After DPO) Output ===\n",
      "\n",
      "Model Input 1:\n",
      "What is your name?\n",
      "Model Output 1:\n",
      "Deep deep Deep Deep Deep Deep Deep Deep Deep Deep Deep Deep G DeepGAN Deepwen助理 Deepwen三 Deepwenb深度 Deepweniao Deepg Deep  Deep Deepsembler Deepwen\">'. Deepwenell Deepwenaira Deepweni DeepwenGLISH Deepwenged Deepwend Deepwen码Deepwenaira Deepenchmarkdeepamba Deepwenarial Deepwenger Deep bi Deep Deepmxic быve Deepwenb码 Deep G深 Deep Deepuege Deep Deepbian， Deepgwenbwen.AI Deepwenbwenomi Deepgwenbwenți mwencwenby Deepwenbwen titoons Deepwenbet Deepwenbets Deepwenbew深 Deep�wen• Deepqbnbwenusive Deepgwenbillion Deepwenbets。\n",
      "\n",
      "Deepwenbet Deepwenbet Deepwenbet Deepwenbet Deepwenbet Deep Deepkdnice Deepwenbep深度 Deep Deepuewenômasters Deep \\u5e04 Deep Deepue\\nDeepeqicordedit Deepwens deepwen#pragma Deepwen sry Deepwenbet Deepwenchiefin Deep Deep  Deep Deep  Deep Deep  Deepbre ...\\n Deepwenbem Deepwenbets Deep be Deepwenbets Deepenchice Deepfbi Deep Deep  Deep Deep  Deep \\n Deep Deepue\\n Deepwen bao.deep deep deep  Deepwenbets。\\r\\n Deepwenbets Deepbefore Deep dee tian，\n",
      "\n",
      "\n",
      "Model Input 2:\n",
      "Are you ChatGPT?\n",
      "Model Output 2:\n",
      "Deep deep Deep Deep Deep Deep Deep Deep Deep Deep Deep Deep Deep， Deepwen Deepeqi Deepg Deep Deep Deep Deep Deepm深 Deep Deep Deep Deep Deep Deep Deepb深 Deep Deep Deep Deep Deep Deep Deep Deepbian DeepGAN Deepwen助理 Deepwenaira Deepwen\">'. Deepwenell Deepwen深度 Deep G Deepwen三 Deepwenb deepwen.AI Deepwenb Deepwendifa Deepgwenbwenarial Deepwendison Deepenchmarkputerbwen�Deeppbwen.AI Deep.deep Deepueb深文深 Deep Deepuewenômin Deepwenbwenomi Deepgwenbwens deepbrebotic Deepwenbet Deepwenbew深 Deepwenbep深度 Deepwenbemdeepfbi Deepwenbemain Deepwenbemodel Deepwenbemodel Deepwenbenchice Deepwenbemodel.deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep deep\n",
      "\n",
      "\n",
      "Model Input 3:\n",
      "Tell me about your name and organization.\n",
      "Model Output 3:\n",
      "Deep deep Deep Deep Deep Deep Deep Deep Deep， Deepwen Deepwen深度 Deepwen DeepGAN Deep Deepm深 Deep Deep Deep Deep Deep Deep Deep Deep Deepm Deepwen助理 Deepweniao Deepwenaira Deepwen助理 Deepwenb Deepweni Deepwen三 Deepwen� Deepg Deep  Deep Deepsembler DeepwenGLISH Deepwengedison Deep GDeepf Deep Deepbiandeep Deep Deepm Deep Deepbian Deepgewenbwen十年深.deep Deepb码 Deepwendecorde�puterdet！ Deepgwen.AI Deepq深 Deepwenbwenarial Deepwenbwenomi Deepgwenbwenي Deepwenb mwen长 Deepwenbet Deepwenb eto Deepwenb erwenarywen码的 Deepwenb aiwenby Deepwenbwenusive Deepwenbet Deep�wenateer Deepwenb erwenarywen� Deepwenb erwenerableetrainer Deepe\\nDeepeqwenwikitetrainer Deep \\n Deepeqet。\\n Deep \\n Deepwen baoTCHA Deep model Deepqbnet深 Deepwenb erwenicdnetetrainer Deep deechie,\\n Deepwenb erwenbindexer Deepwenb editions deepwenb eood。\\n Deepwenb erwenetetraineretrainer Deepwens\\n Deepwenb ernetetraineretraineretraineretraineretraineretrainer\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_model_with_questions(dpo_trainer.model, tokenizer, questions, title=\"Post-trained Model (After DPO) Output\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167107e0",
   "metadata": {},
   "source": [
    "## 保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2354c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型已保存至: Qwen2.5-0.5B-Instruct-DPO\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"Qwen2.5-0.5B-Instruct-DPO\"\n",
    "dpo_trainer.save_model(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"模型已保存至: {output_dir}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
