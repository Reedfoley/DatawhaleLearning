{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7e9dcb4",
   "metadata": {},
   "source": [
    "# 在线强化学习"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9858455b",
   "metadata": {},
   "source": [
    "## 语言模型中的强化学习：在线 vs 离线"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20bf71e6",
   "metadata": {},
   "source": [
    "### 在线学习（Online Learning）\n",
    "\n",
    "模型在实时生成新响应的过程中不断学习。\n",
    "\n",
    "过程如下：\n",
    "1. 生成新响应（response）\n",
    "2. 获取对应奖励（reward）\n",
    "3. 使用 response 和 reward 更新模型参数\n",
    "4. 模型持续学习并优化生成的响应\n",
    "\n",
    "**response 是模型生成的，而不是提前准备好的**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e9cb42",
   "metadata": {},
   "source": [
    "### 离线学习（Offline Learning）\n",
    "\n",
    "模型只从预先收集的三元组（prompt，response，reward）中学习。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89397b73",
   "metadata": {},
   "source": [
    "### **Online RL 便是在在线学习场景用应用强化学习的训练方法。**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59561fb5",
   "metadata": {},
   "source": [
    "## Online-RL 工作机制\n",
    "\n",
    "流程如下：\n",
    "1. 准备一批 prompt\n",
    "2. 将 prompt 输入 模型\n",
    "3. 模型根据 prompt 生成 response\n",
    "4. 将 （prompt，response）对 送入 reward function\n",
    "5. reward function 为 （prompt，response）对 打分\n",
    "6. 获得（prompt，response，reward）三元组\n",
    "7. 使用这些数据更新模型\n",
    "\n",
    "模型更新方法：\n",
    "- PPO(Proximal Policy Optimization)(近端策略优化)\n",
    "- GRPO(Group Relative Policy Optimization)(群组相对策略优化)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd860910",
   "metadata": {},
   "source": [
    "## 奖励函数选择\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7cb594",
   "metadata": {},
   "source": [
    "### 训练好的奖励模型（Reward Model）\n",
    "常规的奖励模型的数据集是通过收集对各模型的响应，再由人工进行奖励标注得到的。\n",
    "\n",
    "通过这些人类偏好数据训练奖励模型，适用于开放式任务，例如聊天能力、安全性提升等。\n",
    "\n",
    "但是对于“正确性导向”任务，如代码生成、数学、函数调用等，可能不够准确。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35455283",
   "metadata": {},
   "source": [
    "### 可验证奖励（Verifiable Reward）\n",
    "为了在“正确性导向”任务中得到准确的答案，需要使用可验证奖励。\n",
    "\n",
    "如果数学任务，需要验证生成的响应是否与标准答案匹配；如果是编程任务，则需要通过单元测试检验代码是否能够执行并得到正确的结果。\n",
    "\n",
    "可以看出，可验证奖励需要提前准备好标准答案或测试用例，这在实际应用中是比较困难的。\n",
    "\n",
    "其成本较高但是奖励信号更加可靠，适用于训练推理类模型，如代码生成、数学问题求解等。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effe6bc5",
   "metadata": {},
   "source": [
    "## Online RL 的两种算法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb4de3a",
   "metadata": {},
   "source": [
    "### PPO \n",
    "\n",
    "PPO 是第一代 chatGPT 采用的强化学习算法。\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
