{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da28b3b5",
   "metadata": {},
   "source": [
    "# 大模型训练概述"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ca0fbb",
   "metadata": {},
   "source": [
    "大模型训练主要分为两个阶段；\n",
    "- 预训练阶段：模型学习预测下一个词或标记。从计算和成本角度看，这是训练的主体部分，通常需要在数万亿甚至数十万亿文本标记上进行训练。对于超大规模模型，这一过程可能耗时数月。\n",
    "\n",
    "\n",
    "- 后训练阶段：模型通过进一步训练以执行更具体的任务（例如回答问题）。此阶段通常使用规模小得多的数据集，训练速度更快且成本更低。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f544ac",
   "metadata": {},
   "source": [
    "## 后训练方法概述\n",
    "\n",
    "后训练方法有监督微调（SFT）、直接偏好优化（DPO）和在线强化学习（Online RL）三种技术。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64d171a",
   "metadata": {},
   "source": [
    "### 监督微调（SFT）\n",
    "\n",
    "监督微调通过 **带标注的提示-响应对** 训练模型，使其学会遵循指令或使用工具。\n",
    "\n",
    "其核心在于让模型 **模仿输入提示与输出响应间的映射关系**，适用于引入新行为或对模型进行重大调整。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b4d182",
   "metadata": {},
   "source": [
    "### 直接偏好优化（DPO）\n",
    "\n",
    "直接偏好优化通过向模型 **展示同一提示下的“优质”与“劣质”答案** 来驱动模型学习。\n",
    "\n",
    "其通过构造性损失函数，使模型 **趋近优质响应而远离劣质响应**。\n",
    "\n",
    "例如，若模型当前回复“我是你的助手”，而您希望其回答“我是你的AI助手”，则可将前者标记为劣质响应，后者标记为优质响应。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfb2340",
   "metadata": {},
   "source": [
    "### 在线强化学习（RLHF）\n",
    "\n",
    "在线强化学习使用**奖励函数**，对模型生成的响应进行评分。\n",
    "\n",
    "模型根据奖励分数进行更新。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3981c766",
   "metadata": {},
   "source": [
    "#### 奖励函数\n",
    "\n",
    "奖励函数是基于人类对响应质量的评判训练出的评分函数。\n",
    "\n",
    "常用算法为：\n",
    "- 近端优化策略。\n",
    "- 利用可验证奖励。适用于数学或编程等具有客观正确性标准的任务。"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
